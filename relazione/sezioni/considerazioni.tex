 
\section{Final observations}\label{sec:observations}
\IEEEPARstart{T}{he} General Language Model studied has been useful to understand that, using the Word Embedding technique as a representation of words, it is more efficient than the basic Language Model, because it allows you to effectively find similar terms with each other through, for example, the Cosine Similarity. Furthermore, through this technique, the semantically similar terms are also considered (i.e. ``ship'' and ``boat'') while in the basic Language Model the terms are compared without any transformation.
The lack of experience with the Lucene Java library and the difficulty in finding a correct implementation to adapt to our case, meant that it took us a long time to understand the most appropriate way to carry out the different steps necessary to reproduce the results of the paper.
The reason why we have not used another library nor another language lies in the fact that we have decided, from the beginning, to faithfully reproduce the work done in the paper. However, this choice does not exclude possible future works in which the work could be implemented through other languages ​​or libraries (e.g. gensim in python).

\begin{thebibliography}{9}

\bibitem{slides:agosti}
M.~Agosti and G.~Silvello, \emph{Slides}\hskip 1em plus
    0.5em minus 0.4em\relax Padua, Italy: academic year 2019-2020.
\bibitem{paper:ganguly}
D.~Ganguly, M.~Mitra, D.~Roy and G.~Jones, \emph{A Word Embedding based Generalized Language Model for Information Retrieval}\hskip 1em plus
    0.5em minus 0.4em\relax Dublin, Ireland; Kolkata, India: SIGIR 2015.
\bibitem{github:ganguly}
D.~Ganguly, \emph{GLM GitHub repository}

\end{thebibliography}
